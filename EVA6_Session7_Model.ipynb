{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA6_Session7_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO97EXv+UH/lhDV8a3fv9yR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharatgirdhar/EVA6_Session7/blob/main/EVA6_Session7_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KwoIY2hAcdO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOAs2CRPAdgK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTVvtPXAiybm"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqfERhxci0lh"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vNormalizationType):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.dropout=nn.Dropout(0.05)\n",
        "\n",
        "#C1        \n",
        "        self.conv0 = nn.Conv2d(3, 24, 3, padding=1,bias=False,dilation=2)\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm0=nn.GroupNorm(3,24)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm0=nn.GroupNorm(1,24)\n",
        "        else:\n",
        "          self.Norm0=nn.BatchNorm2d(24)\n",
        "\n",
        "        self.conv1_1 = nn.Conv2d(24, 32, 3, padding=1,bias=False, dilation=2)#28\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm1=nn.GroupNorm(4,32)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm1=nn.GroupNorm(1,32)\n",
        "        else:\n",
        "          self.Norm1=nn.BatchNorm2d(32)\n",
        "          \n",
        "        self.conv2_1 = nn.Conv2d(32, 64, 3, padding=1,bias=False, dilation=2)#28\n",
        "        \n",
        "        if vNormalizationType==0:\n",
        "          self.Norm2=nn.GroupNorm(8,64)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm2=nn.GroupNorm(1,64)        \n",
        "        else:\n",
        "          self.Norm2=nn.BatchNorm2d(64)\n",
        "\n",
        "        #self.BatchNorm3=nn.BatchNorm2d(16)\n",
        "        #self.pool1 = nn.MaxPool2d(2, 2)#14        \n",
        "        self.conv3_1 = nn.Conv2d(64, 128, 3, bias=False, padding=1, dilation=2)#14\n",
        "\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm3=nn.GroupNorm(16,128)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm3=nn.GroupNorm(1,128)\n",
        "        else:\n",
        "          self.Norm3=nn.BatchNorm2d(128)\n",
        "        \n",
        "        \n",
        "        #self.conv4_1 = nn.Conv2d(128, 128, 3, bias=False, padding=1, dilation=2, groups=128)#12\n",
        "        #self.conv4_2 = nn.Conv2d(128, 256, 1, bias=False)#12\n",
        "        #if vNormalizationType==0:\n",
        "        #  self.Norm4=nn.GroupNorm(32,256)\n",
        "        #elif vNormalizationType==1:\n",
        "        #  self.Norm4=nn.GroupNorm(1,256)\n",
        "        #else:\n",
        "        #  self.Norm4=nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(128, 24, 1, bias=False)#10\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm4=nn.GroupNorm(3,24)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm4=nn.GroupNorm(1,24)\n",
        "        else:\n",
        "          self.Norm4=nn.BatchNorm2d(24)\n",
        "\n",
        "#C2\n",
        "        self.conv5_1 = nn.Conv2d(24, 32, 3, bias=False, padding=1, dilation=2)#28\n",
        "#        self.conv5_2 = nn.Conv2d(24, 32, 1, bias=False)#28\n",
        "        \n",
        "        if vNormalizationType==0:\n",
        "          self.Norm5=nn.GroupNorm(4,32)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm5=nn.GroupNorm(1,32)        \n",
        "        else:\n",
        "          self.Norm5=nn.BatchNorm2d(32)\n",
        "\n",
        "        \n",
        "        self.conv6_1 = nn.Conv2d(32, 64, 3, bias=False, padding=1, dilation=2)#28\n",
        "#        self.conv6_2 = nn.Conv2d(32, 64, 1, bias=False)#28\n",
        "        \n",
        "        if vNormalizationType==0:\n",
        "          self.Norm6=nn.GroupNorm(8,64)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm6=nn.GroupNorm(1,64)        \n",
        "        else:\n",
        "          self.Norm6=nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv7_1 = nn.Conv2d(64, 64, 3, bias=False, padding=1, dilation=2, groups=64)#14\n",
        "        self.conv7_2 = nn.Conv2d(64, 128, 1, bias=False)#14\n",
        "        \n",
        "        if vNormalizationType==0:\n",
        "          self.Norm7=nn.GroupNorm(16,128)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm7=nn.GroupNorm(1,128)\n",
        "        else:\n",
        "          self.Norm7=nn.BatchNorm2d(128)\n",
        "        \n",
        "        #self.conv8_1 = nn.Conv2d(128, 128, 3, bias=False, padding=1, dilation=2, groups=128)#12\n",
        "        #self.conv8_2 = nn.Conv2d(128, 256, 1, bias=False)\n",
        "        #if vNormalizationType==0:\n",
        "        #  self.Norm8=nn.GroupNorm(32,256)\n",
        "        #elif vNormalizationType==1:\n",
        "        #  self.Norm8=nn.GroupNorm(1,256)\n",
        "        #else:\n",
        "        #  self.Norm8=nn.BatchNorm2d(256)\n",
        "\n",
        "        \n",
        "        self.conv8 = nn.Conv2d(128, 24, 1, bias=False)#10\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm8=nn.GroupNorm(3,24)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm8=nn.GroupNorm(1,24)\n",
        "        else:\n",
        "          self.Norm8=nn.BatchNorm2d(24)\n",
        "\n",
        "#C3\n",
        "        self.conv9_1 = nn.Conv2d(24, 32, 3, bias=False, dilation=2,padding=1)#28\n",
        "#        self.conv9_2 = nn.Conv2d(24, 32, 1, bias=False)#28\n",
        "        \n",
        "        if vNormalizationType==0:\n",
        "          self.Norm9=nn.GroupNorm(4,32)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm9=nn.GroupNorm(1,32)        \n",
        "        else:\n",
        "          self.Norm9=nn.BatchNorm2d(32)\n",
        "\n",
        "       \n",
        "        self.conv10_1 = nn.Conv2d(32, 64, 3, bias=False, dilation=2)#28\n",
        "#        self.conv10_2 = nn.Conv2d(32, 64, 1, bias=False)#28\n",
        "        \n",
        "        if vNormalizationType==0:\n",
        "          self.Norm10=nn.GroupNorm(8,64)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm10=nn.GroupNorm(1,64)        \n",
        "        else:\n",
        "          self.Norm10=nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv11_1 = nn.Conv2d(64, 64, 3, bias=False, dilation=2, groups=64)#14\n",
        "        self.conv11_2 = nn.Conv2d(64, 128, 1, bias=False)#14\n",
        "        \n",
        "        if vNormalizationType==0:\n",
        "          self.Norm11=nn.GroupNorm(16,128)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm11=nn.GroupNorm(1,128)\n",
        "        else:\n",
        "          self.Norm11=nn.BatchNorm2d(128)\n",
        "        \n",
        "        #self.conv12_1 = nn.Conv2d(128, 128, 3, bias=False, padding=1, dilation=2, groups=128)#12\n",
        "        #self.conv12_2 = nn.Conv2d(128, 256, 1, bias=False)\n",
        "        #if vNormalizationType==0:\n",
        "        #  self.Norm12=nn.GroupNorm(32,256)\n",
        "        #elif vNormalizationType==1:\n",
        "        #  self.Norm12=nn.GroupNorm(1,256)\n",
        "        #else:\n",
        "        #  self.Norm12=nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv12 = nn.Conv2d(128, 24, 1, bias=False)#10\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm12=nn.GroupNorm(3,24)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm12=nn.GroupNorm(1,24)\n",
        "        else:\n",
        "          self.Norm12=nn.BatchNorm2d(24)\n",
        "\n",
        "#C4\n",
        "        \n",
        "        self.conv13_1 = nn. Conv2d(24, 24, 3, bias=False, groups=24, padding=1)#10\n",
        "        self.conv13_2 = nn. Conv2d(24, 32, 1, bias=False)\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm13=nn.GroupNorm(4,32)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm13=nn.GroupNorm(1,32)\n",
        "        else:\n",
        "          self.Norm13=nn.BatchNorm2d(32)\n",
        "\n",
        "        \n",
        "        self.conv14_1 = nn. Conv2d(32, 32, 3, bias=False, groups=32, padding=1)#10\n",
        "        self.conv14_2 = nn. Conv2d(32, 64, 1, bias=False)\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm14=nn.GroupNorm(8,64)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm14=nn.GroupNorm(1,64)\n",
        "        else:\n",
        "          self.Norm14=nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv15_1 = nn. Conv2d(64, 64, 3, bias=False, groups=64)#10\n",
        "        self.conv15_2 = nn. Conv2d(64, 128, 1, bias=False)\n",
        "        \n",
        "        if vNormalizationType==0:\n",
        "          self.Norm15=nn.GroupNorm(16,128)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm15=nn.GroupNorm(1,128)\n",
        "        else:\n",
        "          self.Norm15=nn.BatchNorm2d(128)\n",
        "\n",
        " #       self.conv16_1 = nn. Conv2d(128, 128, 3, bias=False, groups=128)#10\n",
        " #       self.conv16_2 = nn. Conv2d(128, 256, 1, bias=False)\n",
        " #       if vNormalizationType==0:\n",
        " #         self.Norm16=nn.GroupNorm(32,256)\n",
        " #       elif vNormalizationType==1:\n",
        " #         self.Norm16=nn.GroupNorm(1,256)\n",
        " #       else:\n",
        " #         self.Norm16=nn.BatchNorm2d(256)\n",
        "\n",
        "        self.gap1=nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "#        self.fc1=nn.Linear(in_features=256, out_features=100)\n",
        "        self.fc1=nn.Linear(in_features=128, out_features=10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        #C1\n",
        "        x = self.dropout(F.relu(self.Norm4(self.conv4(self.dropout(F.relu(self.Norm3(self.conv3_1(self.dropout(F.relu(self.Norm2(self.conv2_1(self.dropout(F.relu(self.Norm1(self.conv1_1(self.dropout(F.relu(self.Norm0(self.conv0(x))))))))))))))))))))\n",
        "        #C2\n",
        "        x = self.dropout(F.relu(self.Norm8(self.conv8(self.dropout(F.relu(self.Norm7(self.conv7_2(self.conv7_1(self.dropout(F.relu(self.Norm6(self.conv6_1(self.dropout(F.relu(self.Norm5(self.conv5_1(x)))))))))))))))))\n",
        "        #C3\n",
        "        x = self.dropout(F.relu(self.Norm12(self.conv12(self.dropout(F.relu(self.Norm11(self.conv11_2(self.conv11_1(self.dropout(F.relu(self.Norm10(self.conv10_1(self.dropout(F.relu(self.Norm9(self.conv9_1(x)))))))))))))))))\n",
        "        #C4\n",
        "        x = F.relu(self.Norm15(self.conv15_2(self.conv15_1(self.dropout(F.relu(self.Norm14(self.conv14_2(self.conv14_1(self.dropout(F.relu(self.Norm13(self.conv13_2(self.conv13_1(x))))))))))))))\n",
        "        \n",
        "        \n",
        "        x = self.gap1(x)\n",
        "        \n",
        "\n",
        "        x = x.view(-1, 128)\n",
        "        #print(x.shape)\n",
        "        x=self.fc1(x)\n",
        " #       x=self.fc2(x)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyJpqKvClxw2",
        "outputId": "3b35b45b-ec43-4b1a-83ec-64f813528455"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net(2).to(device)\n",
        "summary(model, input_size=(3, 32, 32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 24, 30, 30]             648\n",
            "       BatchNorm2d-2           [-1, 24, 30, 30]              48\n",
            "           Dropout-3           [-1, 24, 30, 30]               0\n",
            "            Conv2d-4           [-1, 32, 28, 28]           6,912\n",
            "       BatchNorm2d-5           [-1, 32, 28, 28]              64\n",
            "           Dropout-6           [-1, 32, 28, 28]               0\n",
            "            Conv2d-7           [-1, 64, 26, 26]          18,432\n",
            "       BatchNorm2d-8           [-1, 64, 26, 26]             128\n",
            "           Dropout-9           [-1, 64, 26, 26]               0\n",
            "           Conv2d-10          [-1, 128, 24, 24]          73,728\n",
            "      BatchNorm2d-11          [-1, 128, 24, 24]             256\n",
            "          Dropout-12          [-1, 128, 24, 24]               0\n",
            "           Conv2d-13           [-1, 24, 24, 24]           3,072\n",
            "      BatchNorm2d-14           [-1, 24, 24, 24]              48\n",
            "          Dropout-15           [-1, 24, 24, 24]               0\n",
            "           Conv2d-16           [-1, 32, 22, 22]           6,912\n",
            "      BatchNorm2d-17           [-1, 32, 22, 22]              64\n",
            "          Dropout-18           [-1, 32, 22, 22]               0\n",
            "           Conv2d-19           [-1, 64, 20, 20]          18,432\n",
            "      BatchNorm2d-20           [-1, 64, 20, 20]             128\n",
            "          Dropout-21           [-1, 64, 20, 20]               0\n",
            "           Conv2d-22           [-1, 64, 18, 18]             576\n",
            "           Conv2d-23          [-1, 128, 18, 18]           8,192\n",
            "      BatchNorm2d-24          [-1, 128, 18, 18]             256\n",
            "          Dropout-25          [-1, 128, 18, 18]               0\n",
            "           Conv2d-26           [-1, 24, 18, 18]           3,072\n",
            "      BatchNorm2d-27           [-1, 24, 18, 18]              48\n",
            "          Dropout-28           [-1, 24, 18, 18]               0\n",
            "           Conv2d-29           [-1, 32, 16, 16]           6,912\n",
            "      BatchNorm2d-30           [-1, 32, 16, 16]              64\n",
            "          Dropout-31           [-1, 32, 16, 16]               0\n",
            "           Conv2d-32           [-1, 64, 12, 12]          18,432\n",
            "      BatchNorm2d-33           [-1, 64, 12, 12]             128\n",
            "          Dropout-34           [-1, 64, 12, 12]               0\n",
            "           Conv2d-35             [-1, 64, 8, 8]             576\n",
            "           Conv2d-36            [-1, 128, 8, 8]           8,192\n",
            "      BatchNorm2d-37            [-1, 128, 8, 8]             256\n",
            "          Dropout-38            [-1, 128, 8, 8]               0\n",
            "           Conv2d-39             [-1, 24, 8, 8]           3,072\n",
            "      BatchNorm2d-40             [-1, 24, 8, 8]              48\n",
            "          Dropout-41             [-1, 24, 8, 8]               0\n",
            "           Conv2d-42             [-1, 24, 8, 8]             216\n",
            "           Conv2d-43             [-1, 32, 8, 8]             768\n",
            "      BatchNorm2d-44             [-1, 32, 8, 8]              64\n",
            "          Dropout-45             [-1, 32, 8, 8]               0\n",
            "           Conv2d-46             [-1, 32, 8, 8]             288\n",
            "           Conv2d-47             [-1, 64, 8, 8]           2,048\n",
            "      BatchNorm2d-48             [-1, 64, 8, 8]             128\n",
            "          Dropout-49             [-1, 64, 8, 8]               0\n",
            "           Conv2d-50             [-1, 64, 6, 6]             576\n",
            "           Conv2d-51            [-1, 128, 6, 6]           8,192\n",
            "      BatchNorm2d-52            [-1, 128, 6, 6]             256\n",
            "AdaptiveAvgPool2d-53            [-1, 128, 1, 1]               0\n",
            "           Linear-54                   [-1, 10]           1,290\n",
            "================================================================\n",
            "Total params: 192,522\n",
            "Trainable params: 192,522\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 7.20\n",
            "Params size (MB): 0.73\n",
            "Estimated Total Size (MB): 7.94\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:226: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}